{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No results returned from Search Person endpoint.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 273\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_employees\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 273\u001b[0m     df_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 250\u001b[0m, in \u001b[0;36mrun_script\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m page_size     \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Get LinkedIn URL from Search Person endpoint\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m linkedin_url \u001b[38;5;241m=\u001b[39m \u001b[43msearch_employees_one_row_per_employee_dedup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcountry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcountry_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrole_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# city=city_input,\u001b[39;49;00m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# university_name=university_input,\u001b[39;49;00m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# province=province_input,\u001b[39;49;00m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# skills=skills_input,\u001b[39;49;00m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# industry=industry_input,\u001b[39;49;00m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# company_name=company_input,\u001b[39;49;00m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# languages=languages_input,\u001b[39;49;00m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Enrich profile using the LinkedIn URL\u001b[39;00m\n\u001b[1;32m    265\u001b[0m profile_json \u001b[38;5;241m=\u001b[39m enrich_profile(api_key\u001b[38;5;241m=\u001b[39mapi_key, linkedin_url\u001b[38;5;241m=\u001b[39mlinkedin_url)\n",
      "Cell \u001b[0;32mIn[67], line 46\u001b[0m, in \u001b[0;36msearch_employees_one_row_per_employee_dedup\u001b[0;34m(api_key, country, role_title, page_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m results \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo results returned from Search Person endpoint.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m linkedin_url \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinkedin_profile_url\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m linkedin_url:\n",
      "\u001b[0;31mValueError\u001b[0m: No results returned from Search Person endpoint."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "def search_employees_one_row_per_employee_dedup(\n",
    "    api_key: str,\n",
    "    country: str,\n",
    "    role_title: str,\n",
    "    # city: str,\n",
    "    # university_name: str,\n",
    "    # province: str,\n",
    "    # skills: str,\n",
    "    # industry: str,\n",
    "    # company_name: str,\n",
    "    # languages: str,\n",
    "    page_size: str = '1'\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Calls the Proxycurl /search/person endpoint with the given parameters\n",
    "    and returns the first LinkedIn URL found in the response.\n",
    "    Raises ValueError if no results or no linkedin_profile_url found.\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    search_endpoint = 'https://nubela.co/proxycurl/api/v2/search/person'\n",
    "    \n",
    "    search_params = {\n",
    "        'country': country,\n",
    "        'past_role_title': role_title,\n",
    "        'city': city,\n",
    "        'education_school_name': university_name,\n",
    "        'region': province,\n",
    "        'skills': skills,\n",
    "        'industries': industry,\n",
    "        'past_company_name': company_name,\n",
    "        'languages': languages,\n",
    "        'page_size': page_size,\n",
    "        'use_cache': 'if-present'\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(search_endpoint, headers=headers, params=search_params)\n",
    "    data = resp.json()\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "    if not results:\n",
    "        raise ValueError(\"No results returned from Search Person endpoint.\")\n",
    "    \n",
    "    linkedin_url = results[0].get(\"linkedin_profile_url\")\n",
    "    if not linkedin_url:\n",
    "        raise ValueError(\"No linkedin_profile_url found in the search result.\")\n",
    "    \n",
    "    return linkedin_url\n",
    "\n",
    "\n",
    "def enrich_profile(api_key: str, linkedin_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calls the Proxycurl /api/v2/linkedin endpoint to fetch detailed\n",
    "    LinkedIn profile data given a LinkedIn URL.\n",
    "    Returns the JSON (dictionary) with the user's profile data.\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "    \n",
    "    params = {\n",
    "        'linkedin_profile_url': linkedin_url,\n",
    "        'personal_contact_number': 'include',\n",
    "        'personal_email': 'include',\n",
    "        'skills': 'include',\n",
    "        'use_cache': 'if-present',\n",
    "        'fallback_to_cache': 'on-error',\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(api_endpoint, headers=headers, params=params)\n",
    "    data = resp.json()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def flatten_profile_data(profile_data: dict, linkedin_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given the enriched LinkedIn profile JSON and its LinkedIn URL,\n",
    "    return a single flat dictionary with pipe-delimited strings for\n",
    "    experiences, education, skills, certifications, etc.\n",
    "    Each subsequent line in experiences/education is prefixed with \"| \".\n",
    "    \"\"\"\n",
    "\n",
    "    def format_date(date_dict):\n",
    "        \"\"\"Return 'MonthName YYYY' or 'None' if missing/invalid.\"\"\"\n",
    "        if not date_dict or \"year\" not in date_dict or \"month\" not in date_dict:\n",
    "            return \"None\"\n",
    "        y = date_dict[\"year\"]\n",
    "        m = date_dict[\"month\"]\n",
    "        return f\"{calendar.month_name[m]} {y}\"\n",
    "\n",
    "    def compute_duration_months(start_dict, end_dict=None):\n",
    "        \"\"\"\n",
    "        Compute approximate duration in months between start_dict and end_dict.\n",
    "        If end_dict is None, use a 'today' date for demonstration (2025-03-25).\n",
    "        \"\"\"\n",
    "        if not start_dict or \"year\" not in start_dict or \"month\" not in start_dict:\n",
    "            return None\n",
    "        \n",
    "        start_year = start_dict[\"year\"]\n",
    "        start_month = start_dict[\"month\"]\n",
    "        \n",
    "        if end_dict and \"year\" in end_dict and \"month\" in end_dict:\n",
    "            end_year = end_dict[\"year\"]\n",
    "            end_month = end_dict[\"month\"]\n",
    "        else:\n",
    "            today = date(2025, 3, 25)\n",
    "            end_year = today.year\n",
    "            end_month = today.month\n",
    "        \n",
    "        start_total = start_year * 12 + (start_month - 1)\n",
    "        end_total   = end_year   * 12 + (end_month   - 1)\n",
    "        \n",
    "        return end_total - start_total\n",
    "\n",
    "    def multiline_pipe_format(lines):\n",
    "        \"\"\"\n",
    "        Given a list of strings (lines), return a single string\n",
    "        where each line after the first is prefixed by '| '.\n",
    "        \"\"\"\n",
    "        if not lines:\n",
    "            return \"\"\n",
    "        # First line as-is, subsequent lines prefixed with \"| \"\n",
    "        out = [lines[0]]\n",
    "        for line in lines[1:]:\n",
    "            out.append(f\"| {line}\")\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "    # Extract top-level fields\n",
    "    full_name         = profile_data.get(\"full_name\", \"\")\n",
    "    country           = profile_data.get(\"country\", \"\")\n",
    "    country_full_name = profile_data.get(\"country_full_name\", \"\")\n",
    "    # rename \"state\" to \"province\" in final data\n",
    "    province          = profile_data.get(\"state\", \"\")  \n",
    "    city              = profile_data.get(\"city\", \"\")\n",
    "    personal_emails   = profile_data.get(\"personal_emails\", [])\n",
    "    personal_numbers  = profile_data.get(\"personal_numbers\", [])\n",
    "    gender            = profile_data.get(\"gender\")\n",
    "    headline          = profile_data.get(\"headline\", \"\")\n",
    "    summary           = profile_data.get(\"summary\", \"\")\n",
    "    industry          = profile_data.get(\"industry\")\n",
    "    \n",
    "    # experiences, education, etc.\n",
    "    experiences = profile_data.get(\"experiences\", [])\n",
    "    education   = profile_data.get(\"education\", [])\n",
    "    skills_list = profile_data.get(\"skills\", [])\n",
    "    lang_list   = profile_data.get(\"languages\", [])\n",
    "    certs       = profile_data.get(\"certifications\", [])\n",
    "    \n",
    "    # Format experiences with trailing pipe\n",
    "    exp_lines = []\n",
    "    for exp in experiences:\n",
    "        title     = exp.get(\"title\", \"\")\n",
    "        company   = exp.get(\"company\", \"\")\n",
    "        start_d   = exp.get(\"starts_at\")\n",
    "        end_d     = exp.get(\"ends_at\")\n",
    "        \n",
    "        from_str  = format_date(start_d)\n",
    "        to_str    = format_date(end_d)\n",
    "        duration  = compute_duration_months(start_d, end_d)\n",
    "        duration_str = f\"{duration} months\" if duration is not None else \"Unknown\"\n",
    "        \n",
    "        # Note the pipe at the end\n",
    "        line = (\n",
    "            f\"Role: {title} | Company: {company} | From: {from_str} | \"\n",
    "            f\"To: {to_str} | Duration: {duration_str} |\"\n",
    "        )\n",
    "        exp_lines.append(line)\n",
    "    experiences_str = multiline_pipe_format(exp_lines)\n",
    "\n",
    "    # Format education with trailing pipe\n",
    "    edu_lines = []\n",
    "    for edu_item in education:\n",
    "        school  = edu_item.get(\"school\", \"\")\n",
    "        degree  = edu_item.get(\"degree_name\", \"\")\n",
    "        start_  = edu_item.get(\"starts_at\")\n",
    "        end_    = edu_item.get(\"ends_at\")\n",
    "        \n",
    "        from_edu = format_date(start_)\n",
    "        to_edu   = format_date(end_)\n",
    "        \n",
    "        line = (\n",
    "            f\"Institution: {school} | Degree: {degree} | \"\n",
    "            f\"From: {from_edu} | To: {to_edu} |\"\n",
    "        )\n",
    "        edu_lines.append(line)\n",
    "    education_str = multiline_pipe_format(edu_lines)\n",
    "\n",
    "    # Format skills, languages\n",
    "    skills_str    = \" | \".join(skills_list)\n",
    "    languages_str = \" | \".join(lang_list)\n",
    "\n",
    "    # Format certifications\n",
    "    cert_lines = []\n",
    "    for c in certs:\n",
    "        name      = c.get(\"name\", \"\")\n",
    "        authority = c.get(\"authority\", \"\")\n",
    "        start_c   = c.get(\"starts_at\")\n",
    "        end_c     = c.get(\"ends_at\")\n",
    "        from_c    = format_date(start_c)\n",
    "        to_c      = format_date(end_c)\n",
    "        \n",
    "        line = (\n",
    "            f\"Name: {name} | Authority: {authority} | \"\n",
    "            f\"From: {from_c} | To: {to_c} |\"\n",
    "        )\n",
    "        cert_lines.append(line)\n",
    "    certs_str = multiline_pipe_format(cert_lines)\n",
    "\n",
    "    # Build final record as a flat dict\n",
    "    record = {\n",
    "        \"full_name\":          full_name,\n",
    "        \"country\":            country,\n",
    "        \"country_full_name\":  country_full_name,\n",
    "        \"province\":           province,           \n",
    "        \"city\":               city,\n",
    "        \"personal_emails\":    \", \".join(personal_emails),\n",
    "        \"personal_numbers\":   \", \".join(personal_numbers),\n",
    "        \"URL\":                linkedin_url,\n",
    "        \"gender\":             gender,\n",
    "        \"headline\":           headline,\n",
    "        \"summary\":            summary,\n",
    "        \"industry\":           industry,\n",
    "        \"experiences\":        experiences_str,\n",
    "        \"education\":          education_str,\n",
    "        \"skills\":             skills_str,\n",
    "        \"certifications\":     certs_str,\n",
    "        \"languages\":          languages_str\n",
    "    }\n",
    "    return record\n",
    "\n",
    "\n",
    "def run_script() -> pd.DataFrame:\n",
    "    api_key = \"QY5y_O7Axx4-fcXPEiTwmA\"\n",
    "    country_input = \"ZA\"\n",
    "    role_input    = \"Analyst\"\n",
    "    #city_input    = \"Cape Town\"\n",
    "    # university_input = \"\"\n",
    "    # province_input = \"\"\n",
    "    # skills_input = \"\"\n",
    "    # industry_input = \"\"\n",
    "    # company_input = \"\"\n",
    "    # languages_input = \"\"\n",
    "    page_size     = \"1\"\n",
    "\n",
    "    # Get LinkedIn URL from Search Person endpoint\n",
    "    linkedin_url = search_employees_one_row_per_employee_dedup(\n",
    "        api_key=api_key,\n",
    "        country=country_input,\n",
    "        role_title=role_input,\n",
    "        # city=city_input,\n",
    "        # university_name=university_input,\n",
    "        # province=province_input,\n",
    "        # skills=skills_input,\n",
    "        # industry=industry_input,\n",
    "        # company_name=company_input,\n",
    "        # languages=languages_input,\n",
    "        page_size=page_size\n",
    "    )\n",
    "\n",
    "    # Enrich profile using the LinkedIn URL\n",
    "    profile_json = enrich_profile(api_key=api_key, linkedin_url=linkedin_url)\n",
    "    record = flatten_profile_data(profile_data=profile_json, linkedin_url=linkedin_url)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_employees = pd.DataFrame([record])\n",
    "    return df_employees\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_result = run_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "import logging\n",
    "import warnings\n",
    "import re\n",
    "from typing import List, Optional\n",
    "import io\n",
    "import hashlib\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import time\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "# Set up logging and ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# User authentication functions\n",
    "def make_hashed_password(password):\n",
    "    \"\"\"Create a hashed version of the password.\"\"\"\n",
    "    return hashlib.sha256(str.encode(password)).hexdigest()\n",
    "\n",
    "def check_password(stored_password, input_password):\n",
    "    \"\"\"Check if the input password matches the stored password.\"\"\"\n",
    "    return stored_password == make_hashed_password(input_password)\n",
    "\n",
    "SHEET_URL = \"https://docs.google.com/spreadsheets/d/15FFMP8aUFeeb3I43SAYDevJAw0m5v740Pn7rQCUwnUo/edit#gid=0\"\n",
    "\n",
    "@st.cache_resource(show_spinner=\"Initializing Application...\")\n",
    "def get_google_sheets_client(sheet_url, credentials_file=None):\n",
    "    if credentials_file:\n",
    "        scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_name(credentials_file, scope)\n",
    "    else:\n",
    "        creds_json = os.environ.get(\"GOOGLE_CREDS_FILE\")\n",
    "        if not creds_json:\n",
    "            raise ValueError(\"Google credentials not found in environment variables.\")\n",
    "        creds_dict = json.loads(creds_json)\n",
    "        scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    sheet = client.open_by_url(sheet_url).sheet1 \n",
    "    return sheet\n",
    "\n",
    "def init_db(credentials_file=None):\n",
    "    sheet = get_google_sheets_client(SHEET_URL, credentials_file)\n",
    "    try:\n",
    "        if not sheet.get_all_records():\n",
    "            sheet.append_row([\"id\", \"username\", \"email\", \"password\", \"role\", \"created_at\"])\n",
    "            logger.info(\"Google Sheet initialized with headers.\")\n",
    "        \n",
    "        users = load_users(sheet)\n",
    "        if \"admin\" not in users:\n",
    "            save_users(\"admin\", \"SPI123@_\", \"admin@example.com\", \"admin\", sheet)\n",
    "            logger.info(\"Default admin user inserted.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing Google Sheet: {str(e)}\")\n",
    "\n",
    "# Load users from Google Sheet\n",
    "@st.cache_data(ttl=600)  # Cache for 10 minutes\n",
    "def load_users(_sheet): \n",
    "    users = {}\n",
    "    try:\n",
    "        records = _sheet.get_all_records()\n",
    "        for row in records:\n",
    "            try:\n",
    "                created_at = datetime.strptime(row[\"created_at\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "            except:\n",
    "                created_at = datetime.now()  # fallback if date parsing fails\n",
    "            \n",
    "            users[row[\"username\"]] = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"email\": row[\"email\"],\n",
    "                \"password\": row[\"password\"],\n",
    "                \"role\": row[\"role\"],\n",
    "                \"created_at\": created_at,\n",
    "            }\n",
    "        logger.info(f\"Loaded {len(users)} users from Google Sheet\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading users from Google Sheet: {str(e)}\")\n",
    "    return users\n",
    "\n",
    "def save_users(username, password, email, role, sheet):\n",
    "    try:\n",
    "        users = load_users(sheet) \n",
    "        if username in users:\n",
    "            cell = sheet.find(username)\n",
    "            sheet.update_cell(cell.row, 3, email)\n",
    "            sheet.update_cell(cell.row, 4, make_hashed_password(password))\n",
    "            sheet.update_cell(cell.row, 5, role)\n",
    "        else:\n",
    "            next_id = len(users) + 1\n",
    "            sheet.append_row([next_id, username, email, make_hashed_password(password), role, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")])\n",
    "        logger.info(f\"User '{username}' saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving user to Google Sheet: {str(e)}\")\n",
    "\n",
    "\n",
    "def delete_user(username, sheet):\n",
    "    try:\n",
    "        cell = sheet.find(username)  # Find the row with the username\n",
    "        sheet.delete_rows(cell.row)  # Corrected method\n",
    "        logger.info(f\"User '{username}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error deleting user from Google Sheet: {str(e)}\")\n",
    "\n",
    "\n",
    "def search_employees_one_row_per_employee_dedup(\n",
    "    query,\n",
    "    country_filter=None,\n",
    "    location_filter=None,\n",
    "    university_filter=None,\n",
    "    province_filter=None,\n",
    "    skills_filter=None,\n",
    "    industry_filter=None,\n",
    "    company_filter=None,\n",
    "    languages_filter=None,\n",
    "    max_to_fetch=None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Calls the Proxycurl /search/person endpoint with the given parameters\n",
    "    and returns the first LinkedIn URL found in the response.\n",
    "    Raises ValueError if no results or no linkedin_profile_url found.\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer etYNX6MUmWK84R7B9NXBZw'}\n",
    "    search_endpoint = 'https://nubela.co/proxycurl/api/v2/search/person'\n",
    "    \n",
    "    search_params = {\n",
    "        'past_role_title': query,\n",
    "        'country': country_filter,\n",
    "        'city': location_filter,\n",
    "        'education_school_name': university_filter,\n",
    "        'region': province_filter,\n",
    "        'skills': skills_filter,\n",
    "        'industries': industry_filter,\n",
    "        'past_company_name': company_filter,\n",
    "        'languages': languages_filter,\n",
    "        'page_size': max_to_fetch,\n",
    "        'use_cache': 'if-present'\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(search_endpoint, headers=headers, params=search_params)\n",
    "    data = resp.json()\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "    if not results:\n",
    "        raise ValueError(\"No results returned from Search Person endpoint.\")\n",
    "    \n",
    "    linkedin_url = results[0].get(\"linkedin_profile_url\")\n",
    "    if not linkedin_url:\n",
    "        raise ValueError(\"No linkedin_profile_url found in the search result.\")\n",
    "    \n",
    "    return linkedin_url\n",
    "\n",
    "\n",
    "def enrich_profile(linkedin_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calls the Proxycurl /api/v2/linkedin endpoint to fetch detailed\n",
    "    LinkedIn profile data given a LinkedIn URL.\n",
    "    Returns the JSON (dictionary) with the user's profile data.\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer etYNX6MUmWK84R7B9NXBZw'}\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "    \n",
    "    params = {\n",
    "        'linkedin_profile_url': linkedin_url,\n",
    "        'personal_contact_number': 'include',\n",
    "        'personal_email': 'include',\n",
    "        'skills': 'include',\n",
    "        'use_cache': 'if-present',\n",
    "        'fallback_to_cache': 'on-error',\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(api_endpoint, headers=headers, params=params)\n",
    "    data = resp.json()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def flatten_profile_data(profile_data: dict, linkedin_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given the enriched LinkedIn profile JSON and its LinkedIn URL,\n",
    "    return a single flat dictionary with pipe-delimited strings for\n",
    "    experiences, education, skills, certifications, etc.\n",
    "    Each subsequent line in experiences/education is prefixed with \"| \".\n",
    "    \"\"\"\n",
    "    def format_date(date_dict):\n",
    "        \"\"\"Return 'MonthName YYYY' or 'None' if missing/invalid.\"\"\"\n",
    "        if not date_dict or \"year\" not in date_dict or \"month\" not in date_dict:\n",
    "            return \"None\"\n",
    "        return f\"{calendar.month_name[date_dict['month']]} {date_dict['year']}\"\n",
    "\n",
    "    def compute_duration_months(start_dict, end_dict=None):\n",
    "        \"\"\"Compute duration in months between dates.\"\"\"\n",
    "        if not start_dict or \"year\" not in start_dict or \"month\" not in start_dict:\n",
    "            return None\n",
    "        today = date(2025, 3, 25)  # Fixed date for consistency\n",
    "        start = start_dict[\"year\"] * 12 + (start_dict[\"month\"] - 1)\n",
    "        end = (end_dict[\"year\"] * 12 + (end_dict[\"month\"] - 1) if end_dict \n",
    "               else today.year * 12 + (today.month - 1))\n",
    "        return end - start\n",
    "\n",
    "    def format_experience(exp):\n",
    "        \"\"\"Format a single experience entry.\"\"\"\n",
    "        title = exp.get(\"title\", \"\")\n",
    "        company = exp.get(\"company\", \"\")\n",
    "        start = format_date(exp.get(\"starts_at\"))\n",
    "        end = format_date(exp.get(\"ends_at\"))\n",
    "        duration = compute_duration_months(exp.get(\"starts_at\"), exp.get(\"ends_at\"))\n",
    "        duration_str = f\"{duration} months\" if duration else \"Unknown\"\n",
    "        return (f\"Role: {title} | Company: {company} | From: {start} | \"\n",
    "                f\"To: {end} | Duration: {duration_str} |\")\n",
    "\n",
    "    def format_education(edu):\n",
    "        \"\"\"Format a single education entry.\"\"\"\n",
    "        school = edu.get(\"school\", \"\")\n",
    "        degree = edu.get(\"degree_name\", \"\")\n",
    "        start = format_date(edu.get(\"starts_at\"))\n",
    "        end = format_date(edu.get(\"ends_at\"))\n",
    "        return (f\"Institution: {school} | Degree: {degree} | \"\n",
    "                f\"From: {start} | To: {end} |\")\n",
    "\n",
    "    def format_certification(cert):\n",
    "        \"\"\"Format a single certification entry.\"\"\"\n",
    "        name = cert.get(\"name\", \"\")\n",
    "        authority = cert.get(\"authority\", \"\")\n",
    "        start = format_date(cert.get(\"starts_at\"))\n",
    "        end = format_date(cert.get(\"ends_at\")) if cert.get(\"ends_at\") else \"Present\"\n",
    "        return (f\"Name: {name} | Authority: {authority} | \"\n",
    "                f\"From: {start} | To: {end} |\")\n",
    "\n",
    "    # Handle languages field which might be a string or list of dicts\n",
    "    languages = profile_data.get(\"languages\", [])\n",
    "    if isinstance(languages, str):\n",
    "        languages_str = languages\n",
    "    elif isinstance(languages, list):\n",
    "        try:\n",
    "            languages_str = \" | \".join(\n",
    "                lang.get(\"name\", \"\") if isinstance(lang, dict) else str(lang)\n",
    "                for lang in languages\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing languages: {str(e)}\")\n",
    "            languages_str = \"\"\n",
    "    else:\n",
    "        languages_str = str(languages)\n",
    "\n",
    "    # Extract top-level fields\n",
    "    record = {\n",
    "        \"full_name\": profile_data.get(\"full_name\", \"\"),\n",
    "        \"country\": profile_data.get(\"country\", \"\"),\n",
    "        \"country_full_name\": profile_data.get(\"country_full_name\", \"\"),\n",
    "        \"province\": profile_data.get(\"state\", \"\"),\n",
    "        \"city\": profile_data.get(\"city\", \"\"),\n",
    "        \"personal_emails\": \", \".join(profile_data.get(\"personal_emails\", [])),\n",
    "        \"personal_numbers\": \", \".join(profile_data.get(\"personal_numbers\", [])),\n",
    "        \"URL\": linkedin_url,\n",
    "        \"gender\": profile_data.get(\"gender\"),\n",
    "        \"headline\": profile_data.get(\"headline\", \"\"),\n",
    "        \"summary\": profile_data.get(\"summary\", \"\"),\n",
    "        \"industry\": profile_data.get(\"industry\"),\n",
    "        \"experiences\": \"\\n\".join([format_experience(exp) for exp in profile_data.get(\"experiences\", [])]),\n",
    "        \"education\": \"\\n\".join([format_education(edu) for edu in profile_data.get(\"education\", [])]),\n",
    "        \"skills\": \" | \".join(profile_data.get(\"skills\", [])),\n",
    "        \"certifications\": \"\\n\".join([format_certification(cert) for cert in profile_data.get(\"certifications\", [])]),\n",
    "        \"languages\": languages_str\n",
    "    }\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_script(\n",
    "    query: str,\n",
    "    country_filter=None,\n",
    "    location_filter=None,\n",
    "    university_filter=None,\n",
    "    province_filter=None,\n",
    "    skills_filter=None,\n",
    "    industry_filter=None,\n",
    "    company_filter=None,\n",
    "    languages_filter=None,\n",
    "    max_to_fetch=None\n",
    ") -> pd.DataFrame:\n",
    "    try:\n",
    "        linkedin_url = search_employees_one_row_per_employee_dedup(\n",
    "            query,\n",
    "            country_filter=country_filter,\n",
    "            location_filter=location_filter,\n",
    "            university_filter=university_filter,\n",
    "            province_filter=province_filter,\n",
    "            skills_filter=skills_filter,\n",
    "            industry_filter=industry_filter,\n",
    "            company_filter=company_filter,\n",
    "            languages_filter=languages_filter,\n",
    "            max_to_fetch=max_to_fetch\n",
    "        )\n",
    "    except ValueError:\n",
    "        # This happens if no results or no LinkedIn URL is found\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "\n",
    "    profile_json = enrich_profile(linkedin_url=linkedin_url)\n",
    "    record = flatten_profile_data(profile_data=profile_json, linkedin_url=linkedin_url)\n",
    "    \n",
    "    df_employees = pd.DataFrame([record])\n",
    "    return df_employees\n",
    "\n",
    "# Ranking functions\n",
    "def build_user_text(row, text_columns: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Combine relevant text fields into a single string for semantic comparison.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for col in text_columns:\n",
    "        val = row.get(col)\n",
    "        if pd.notnull(val):\n",
    "            if isinstance(val, list):\n",
    "                parts.append(' '.join(map(str, val)))\n",
    "            else:\n",
    "                parts.append(str(val))\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize text input.\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  \n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  \n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.strip().split())\n",
    "    return text\n",
    "\n",
    "def rank_candidates_semantic(\n",
    "    df_employees: pd.DataFrame,\n",
    "    job_description: str,\n",
    "    text_columns: Optional[List[str]] = None,\n",
    "    model_name: str = 'all-MiniLM-L6-v2',\n",
    "    batch_size: int = 32\n",
    ") -> pd.DataFrame:\n",
    "    try:\n",
    "        logger.info(\"Starting candidate ranking process...\")\n",
    "        df = df_employees.copy()\n",
    "        \n",
    "        if text_columns is None:\n",
    "            text_columns = [\n",
    "                'summary', 'experiences', 'education', 'headline',\n",
    "                'industry', 'skills', 'certifications'\n",
    "            ]\n",
    "            \n",
    "            logger.debug(f\"Using default text columns: {text_columns}\")\n",
    "        else:\n",
    "            logger.debug(f\"Using custom text columns: {text_columns}\")\n",
    "\n",
    "        logger.info(\"Combining candidate text fields...\")\n",
    "        df['combined_text'] = df.apply(lambda x: build_user_text(x, text_columns), axis=1)\n",
    "        logger.info(f\"Processed {len(df)} candidate profiles\")\n",
    "\n",
    "        logger.info(\"Filtering empty candidate texts...\")\n",
    "        initial_count = len(df)\n",
    "        df['combined_text'] = df['combined_text'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "        df = df.dropna(subset=['combined_text']).reset_index(drop=True)\n",
    "        filtered_count = len(df)\n",
    "        logger.info(f\"Removed {initial_count - filtered_count} empty profiles, {filtered_count} remaining\")\n",
    "\n",
    "        if df.empty:\n",
    "            logger.warning(\"No valid candidate texts found after preprocessing\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        logger.info(f\"Initializing sentence transformer model: {model_name}\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "        \n",
    "        logger.info(\"Preprocessing job description...\")\n",
    "        clean_jd = preprocess_text(job_description)\n",
    "        logger.debug(f\"Job description length: {len(clean_jd.split())} words\")\n",
    "        \n",
    "        logger.info(\"Encoding job description...\")\n",
    "        job_embedding = model.encode(clean_jd, convert_to_tensor=True)\n",
    "        logger.debug(f\"Job embedding shape: {job_embedding.shape}\")\n",
    "\n",
    "        logger.info(\"Preprocessing candidate texts...\")\n",
    "        user_texts = df['combined_text'].apply(preprocess_text).tolist()\n",
    "        logger.debug(f\"First candidate text preview: {user_texts[0][:200]}...\")\n",
    "        \n",
    "        logger.info(f\"Encoding candidate texts in batches of {batch_size}...\")\n",
    "        user_embeddings = model.encode(\n",
    "            user_texts,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        logger.info(f\"Successfully encoded {len(user_texts)} candidate texts\")\n",
    "        logger.debug(f\"Embeddings matrix shape: {user_embeddings.shape}\")\n",
    "\n",
    "        logger.info(\"Calculating cosine similarities...\")\n",
    "        similarities = util.cos_sim(job_embedding, user_embeddings)\n",
    "        df['similarity_score'] = similarities.cpu().numpy().flatten()\n",
    "        df['match_percentage'] = (df['similarity_score'] * 100).round(2).astype(str) + '%'\n",
    "        \n",
    "        min_score = df['similarity_score'].min()\n",
    "        max_score = df['similarity_score'].max()\n",
    "        logger.info(f\"Similarity scores range: {min_score:.3f} - {max_score:.3f}\")\n",
    "        logger.debug(f\"Score distribution:\\n{df['similarity_score'].describe()}\")\n",
    "\n",
    "        logger.info(\"Sorting candidates by similarity score...\")\n",
    "        df_sorted = df.sort_values(by='similarity_score', ascending=False).reset_index(drop=True)\n",
    "        df_sorted = df_sorted.drop('combined_text', axis=1)\n",
    "\n",
    "        logger.info(f\"Top candidate score: {df_sorted.iloc[0]['similarity_score']:.3f}\")\n",
    "        logger.info(\"Ranking process completed successfully\")\n",
    "        return df_sorted\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ranking candidates: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Cache the model to avoid reloading\n",
    "@st.cache_resource\n",
    "def load_model(model_name='all-MiniLM-L6-v2'):\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "# Function to convert dataframe to Excel for download\n",
    "def to_excel(df):\n",
    "    \"\"\"Robust Excel export that handles all data types and empty values\"\"\"\n",
    "    output = io.BytesIO()\n",
    "    \n",
    "    # Create a clean copy of the DataFrame\n",
    "    export_df = df.copy()\n",
    "    \n",
    "    # Convert all columns to string and handle None/NaN values\n",
    "    for col in export_df.columns:\n",
    "        # Replace None/NaN with empty string\n",
    "        export_df[col] = export_df[col].fillna('')\n",
    "        # Convert all values to string\n",
    "        export_df[col] = export_df[col].astype(str)\n",
    "        # Remove any problematic characters\n",
    "        export_df[col] = export_df[col].str.replace('\\x00', '')  # Remove null bytes\n",
    "    \n",
    "    # Create Excel file with xlsxwriter\n",
    "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "        export_df.to_excel(writer, sheet_name='Candidates', index=False)\n",
    "        \n",
    "        # Auto-adjust column widths\n",
    "        worksheet = writer.sheets['Candidates']\n",
    "        for idx, col in enumerate(export_df.columns):\n",
    "            max_len = max((\n",
    "                export_df[col].astype(str).map(len).max(),\n",
    "                len(str(col))\n",
    "            )) + 1\n",
    "            worksheet.set_column(idx, idx, min(max_len, 50))\n",
    "    \n",
    "    output.seek(0)\n",
    "    return output.getvalue()\n",
    "\n",
    "# Login management functions\n",
    "def login_page():\n",
    "    st.title(\"SPI Executive Search\")\n",
    "    login_tab, signup_tab, forgot_tab = st.tabs([\"Login\", \"Sign Up\", \"Forgot Password\"])\n",
    "\n",
    "    sheet = get_google_sheets_client(SHEET_URL, credentials_file=GOOGLE_CREDS_FILE) \n",
    "\n",
    "    with login_tab:\n",
    "        username_or_email = st.text_input(\"Username or Email\", key=\"login_username_email\")\n",
    "        password = st.text_input(\"Password\", type=\"password\", key=\"login_password\")\n",
    "        \n",
    "        if st.button(\"Login\"):\n",
    "            users = load_users(sheet) \n",
    "            \n",
    "            if username_or_email in users:\n",
    "                if check_password(users[username_or_email]['password'], password):\n",
    "                    st.session_state.logged_in = True\n",
    "                    st.session_state.username = username_or_email\n",
    "                    st.session_state.user_role = users[username_or_email].get('role', 'user')\n",
    "                    st.success(f\"Welcome back, {username_or_email}!\")\n",
    "                    st.rerun()\n",
    "                else:\n",
    "                    st.error(\"Invalid password.\")\n",
    "            else:\n",
    "                found = False\n",
    "                for username, user_data in users.items():\n",
    "                    if user_data.get('email') == username_or_email:\n",
    "                        if check_password(user_data['password'], password):\n",
    "                            st.session_state.logged_in = True\n",
    "                            st.session_state.username = username\n",
    "                            st.session_state.user_role = user_data.get('role', 'user')\n",
    "                            st.success(f\"Welcome back, {username}!\")\n",
    "                            found = True\n",
    "                            st.rerun()\n",
    "                            break\n",
    "                        else:\n",
    "                            st.error(\"Invalid password.\")\n",
    "                            found = True\n",
    "                            break\n",
    "                \n",
    "                if not found:\n",
    "                    st.error(\"Invalid username/email or password.\")\n",
    "    \n",
    "    with signup_tab:\n",
    "        if st.session_state.get('user_role') == 'admin':\n",
    "            new_username = st.text_input(\"New Username\", key=\"new_username\")\n",
    "            new_password = st.text_input(\"New Password\", type=\"password\", key=\"new_password\")\n",
    "            confirm_password = st.text_input(\"Confirm Password\", type=\"password\", key=\"confirm_password\")\n",
    "            email = st.text_input(\"Email\", key=\"email\")\n",
    "            \n",
    "            if st.button(\"Sign Up\"):\n",
    "                users = load_users(sheet)  # Pass the sheet object\n",
    "                # Check if username already exists\n",
    "                if new_username in users:\n",
    "                    st.error(\"Username already exists\")\n",
    "                # Check if email already exists\n",
    "                elif any(user_data.get('email') == email for user_data in users.values()):\n",
    "                    st.error(\"Email already in use\")\n",
    "                elif new_password != confirm_password:\n",
    "                    st.error(\"Passwords do not match\")\n",
    "                elif not new_username or not new_password:\n",
    "                    st.error(\"Username and password cannot be empty\")\n",
    "                elif not email:\n",
    "                    st.error(\"Email cannot be empty\")\n",
    "                else:\n",
    "                    # Save the new user to the database\n",
    "                    save_users(new_username, new_password, email, 'user', sheet)  # Added sheet parameter\n",
    "                    st.success(\"Account created successfully! You can now login.\")\n",
    "        else:\n",
    "            st.info(\"User registration is only managed by administrators. Please contact your administrator for access.\")\n",
    "\n",
    "    with forgot_tab:\n",
    "        st.subheader(\"Reset Password\")\n",
    "        username_or_email = st.text_input(\"Enter your username or email\", key=\"reset_username_email\")\n",
    "        new_password = st.text_input(\"New Password\", type=\"password\", key=\"reset_new_password\")\n",
    "        confirm_password = st.text_input(\"Confirm New Password\", type=\"password\", key=\"reset_confirm_password\")\n",
    "        \n",
    "        if st.button(\"Reset Password\", key=\"reset_password_button\"):\n",
    "            users = load_users(sheet)  # Pass the sheet object\n",
    "            user_found = False\n",
    "            username = None\n",
    "            \n",
    "            # Check if the input is a username\n",
    "            if username_or_email in users:\n",
    "                username = username_or_email\n",
    "                user_found = True\n",
    "            else:\n",
    "                # Check if the input is an email\n",
    "                for u, data in users.items():\n",
    "                    if data.get('email', '').lower() == username_or_email.lower():\n",
    "                        username = u\n",
    "                        user_found = True\n",
    "                        break\n",
    "            \n",
    "            if not user_found:\n",
    "                st.error(\"No account found with the provided username or email.\")\n",
    "            elif new_password != confirm_password:\n",
    "                st.error(\"New passwords do not match. Please try again.\")\n",
    "            else:\n",
    "                # Update the password\n",
    "                hashed_password = make_hashed_password(new_password)  # Hash the new password\n",
    "                save_users(username, new_password, users[username]['email'], users[username]['role'], sheet)\n",
    "                \n",
    "                # Clear the cache to ensure fresh data is loaded\n",
    "                st.cache_data.clear()\n",
    "                st.session_state.user_cache_clear = True\n",
    "                \n",
    "                st.success(\"Password has been reset successfully. You can now login with your new password.\")\n",
    "                time.sleep(2)  # Show message for 2 seconds\n",
    "                st.rerun()\n",
    "\n",
    "\n",
    "def logout():\n",
    "    if st.sidebar.button(\"Logout\"):\n",
    "        for key in ['logged_in', 'username', 'user_role']:\n",
    "            if key in st.session_state:\n",
    "                del st.session_state[key]\n",
    "        st.rerun()\n",
    "\n",
    "\n",
    "def admin_dashboard():\n",
    "    st.title(\"Admin Dashboard - User Management\")\n",
    "    sheet = get_google_sheets_client(SHEET_URL, credentials_file=GOOGLE_CREDS_FILE)\n",
    "    \n",
    "    # Clear the cache before loading users to ensure we get fresh data\n",
    "    if 'user_cache_clear' in st.session_state:\n",
    "        st.cache_data.clear()\n",
    "        del st.session_state.user_cache_clear\n",
    "    \n",
    "    users = load_users(sheet)  # Pass the sheet object\n",
    "    user_df = pd.DataFrame([\n",
    "        {\n",
    "            'Username': username,\n",
    "            'Email': data['email'],\n",
    "            'Created At': data['created_at'].strftime('%Y-%m-%d %H:%M:%S'),  \n",
    "            'Role': data.get('role', 'user')\n",
    "        }\n",
    "        for username, data in users.items()\n",
    "    ])\n",
    "    st.dataframe(user_df)\n",
    "    \n",
    "    st.subheader(\"Add New User\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        new_username = st.text_input(\"Username\", key=\"admin_new_username\")\n",
    "        new_password = st.text_input(\"Password\", type=\"password\", key=\"admin_new_password\")\n",
    "    with col2:\n",
    "        email = st.text_input(\"Email\", key=\"admin_email\")\n",
    "        role = st.selectbox(\"Role\", [\"user\", \"admin\"], key=\"admin_role\")\n",
    "    \n",
    "    if st.button(\"Add User\"):\n",
    "        if new_username in users:\n",
    "            st.error(\"Username already exists\")\n",
    "        elif not new_username or not new_password:\n",
    "            st.error(\"Username and password cannot be empty\")\n",
    "        else:\n",
    "            save_users(new_username, new_password, email, role, sheet)\n",
    "            # Set flag to clear cache on next run\n",
    "            st.session_state.user_cache_clear = True\n",
    "            st.success(f\"User '{new_username}' added successfully\")\n",
    "            st.rerun()\n",
    "    \n",
    "    st.subheader(\"Delete User\")\n",
    "    username_to_delete = st.selectbox(\"Select User to Delete\", list(users.keys()))\n",
    "    if st.button(\"Delete User\") and username_to_delete:\n",
    "        if username_to_delete == st.session_state.username:\n",
    "            st.error(\"You cannot delete your own account while logged in!\")\n",
    "        else:\n",
    "            delete_user(username_to_delete, sheet)\n",
    "            # Set flag to clear cache on next run\n",
    "            st.session_state.user_cache_clear = True\n",
    "            st.success(f\"User '{username_to_delete}' deleted successfully\")\n",
    "            st.rerun()\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Candidate Search & Match\", layout=\"wide\")\n",
    "\n",
    "     # Initialize Google Sheets:\n",
    "    sheet = get_google_sheets_client(SHEET_URL, credentials_file=GOOGLE_CREDS_FILE)\n",
    "\n",
    "    if 'db_initialized' not in st.session_state:\n",
    "        init_db()\n",
    "        st.session_state.db_initialized = True\n",
    "    if 'logged_in' not in st.session_state:\n",
    "        st.session_state.logged_in = False\n",
    "    if 'search_results' not in st.session_state:\n",
    "        st.session_state.search_results = None\n",
    "    if 'ranked_results' not in st.session_state:\n",
    "        st.session_state.ranked_results = None\n",
    "    \n",
    "    if st.session_state.logged_in:\n",
    "        st.sidebar.write(f\"Logged in as: **{st.session_state.username}**\")\n",
    "        st.sidebar.write(f\"Role: **{st.session_state.user_role}**\")\n",
    "        logout()\n",
    "        if st.session_state.user_role == 'admin':\n",
    "            pages = [\"Candidate Search\", \"Admin Dashboard\"]\n",
    "            selected_page = st.sidebar.selectbox(\"Navigation\", pages)\n",
    "            if selected_page == \"Admin Dashboard\":\n",
    "                admin_dashboard()\n",
    "                return\n",
    "\n",
    "    if not st.session_state.logged_in:\n",
    "        login_page()\n",
    "        return\n",
    "\n",
    "    st.title(\"Candidate Search & Match\")\n",
    "    st.markdown(\"Find and rank the best candidates for a job position\")\n",
    "    tab1, tab2 = st.tabs([\"Search Candidates\", \"Ranked Results\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.header(\"Search for Candidates\")\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.subheader(\"Search Criteria\")\n",
    "            search_query = st.text_input(\"Job Title/Position\", placeholder=\"e.g. Chief Financial Officer OR CFO\")\n",
    "            loc_col1, loc_col2 = st.columns(2)\n",
    "            with loc_col1:\n",
    "                country = st.text_input(\"Country\", placeholder=\"e.g. ZA\")\n",
    "            with loc_col2:\n",
    "                location = st.text_input(\"City\", placeholder=\"e.g. Johannesburg\")\n",
    "            comp_col1, comp_col2 = st.columns(2)\n",
    "            with comp_col1:\n",
    "                company_filter = st.text_input(\"Company Name\", placeholder=\"e.g. PWC\")\n",
    "            with comp_col2:\n",
    "                university_filter = st.text_input(\"University Name\", placeholder=\"e.g. University of Cape Town\")\n",
    "            industry_col1, industry_col2 = st.columns(2)\n",
    "            with industry_col1:\n",
    "                industry_filter = st.text_input(\"Industry\", placeholder=\"e.g. Accounting\")\n",
    "            with industry_col2:\n",
    "                skills_filter = st.text_input(\"Skills\", placeholder=\"e.g. business strategy, financial modeling\")\n",
    "            cert_col1, cert_col2 = st.columns(2)\n",
    "            with cert_col1:\n",
    "                certifications_filter = st.text_input(\"Certifications\", placeholder=\"e.g. Assessor\")\n",
    "            with cert_col2:\n",
    "                languages_filter = st.text_input(\"Languages\", placeholder=\"e.g. English\")\n",
    "            slider_col, btn_col = st.columns([2, 1])\n",
    "            with slider_col:\n",
    "                max_results = st.slider(\"Maximum number of results\", 1, 150, 15)\n",
    "            with btn_col:\n",
    "                st.write(\"\")\n",
    "                st.write(\"\")\n",
    "                search_button = st.button(\"Search Candidates\")\n",
    "            \n",
    "            if search_button and search_query:\n",
    "                with st.spinner(\"Searching for candidates...\"):\n",
    "                    st.session_state.ranked_results = None\n",
    "                    results = run_script(\n",
    "                        query=search_query,\n",
    "                        country_filter=country if country else None,\n",
    "                        location_filter=location if location else None,\n",
    "                        company_filter=company_filter if company_filter else None, \n",
    "                        university_filter=university_filter if university_filter else None,\n",
    "                        industry_filter=industry_filter if industry_filter else None,\n",
    "                        skills_filter=skills_filter if skills_filter else None,\n",
    "                        # certifications_filter=certifications_filter if certifications_filter else None,\n",
    "                        languages_filter=languages_filter if languages_filter else None,\n",
    "                        max_to_fetch=max_results\n",
    "                    )\n",
    "                    if results.empty:\n",
    "                        st.error(\"No candidates found matching your criteria.\")\n",
    "                    else:\n",
    "                        st.session_state.search_results = results\n",
    "                        st.success(f\"Found {len(results)} candidates!\")\n",
    "        \n",
    "        with col2:\n",
    "            st.subheader(\"Job Description\")\n",
    "            st.markdown(\"Provide a detailed job description to rank candidates against:\")\n",
    "            job_description = st.text_area(\n",
    "                \"Enter job description\", \n",
    "                height=250,\n",
    "                placeholder=\"Paste detailed job description here to rank candidates by relevance...\"\n",
    "            )\n",
    "            rank_button = st.button(\"Rank Candidates\")\n",
    "            \n",
    "            if rank_button:\n",
    "                if st.session_state.search_results is None or st.session_state.search_results.empty:\n",
    "                    st.error(\"Please search for candidates first before ranking.\")\n",
    "                elif not job_description:\n",
    "                    st.warning(\"Please provide a job description for ranking candidates.\")\n",
    "                else:\n",
    "                    with st.spinner(\"Ranking candidates...\"):\n",
    "                        load_model()\n",
    "                        ranked_df = rank_candidates_semantic(\n",
    "                            df_employees=st.session_state.search_results,\n",
    "                            job_description=job_description,\n",
    "                            model_name='all-MiniLM-L6-v2'\n",
    "                        )\n",
    "                        if ranked_df.empty:\n",
    "                            st.error(\"Error occurred during ranking. Please try again.\")\n",
    "                        else:\n",
    "                            st.session_state.ranked_results = ranked_df\n",
    "                            st.success(\"Candidates ranked successfully! View results in the 'Ranked Results' tab.\")\n",
    "        \n",
    "        if st.session_state.search_results is not None and not st.session_state.search_results.empty:\n",
    "            st.subheader(\"Search Results\")\n",
    "            \n",
    "            # Add download button for raw search results\n",
    "            raw_export_df = st.session_state.search_results.copy()\n",
    "            raw_excel_data = to_excel(raw_export_df)\n",
    "            st.download_button(\n",
    "                label=\" Download Search Results (Excel)\",\n",
    "                data=raw_excel_data,\n",
    "                file_name='search_results.xlsx',\n",
    "                mime='application/vnd.ms-excel',\n",
    "                key='raw_download'\n",
    "            )\n",
    "            \n",
    "            for i, row in st.session_state.search_results.iterrows():\n",
    "                with st.expander(f\"{row['full_name']} - {row['headline']}\"):\n",
    "                    col1, col2 = st.columns([1, 2])\n",
    "                    with col1:\n",
    "                        st.markdown(f\"**Location:** {row['city']}, {row['country']}\")\n",
    "                        st.markdown(f\"**Province:** {row['province']}\")\n",
    "                        st.markdown(f\"**Industry:** {row['industry']}\")\n",
    "                        st.markdown(f\"**Profile URL:** [Link]({row['URL']})\")\n",
    "                        if pd.notnull(row['personal_emails']) and row['personal_emails']:\n",
    "                            st.markdown(f\"**Email:** {row['personal_emails']}\")\n",
    "                        if pd.notnull(row['personal_numbers']) and row['personal_numbers']:\n",
    "                            st.markdown(f\"**Phone:** {row['personal_numbers']}\")\n",
    "                    with col2:\n",
    "                        if pd.notnull(row['summary']) and row['summary']:\n",
    "                            st.markdown(\"**Summary:**\")\n",
    "                            st.markdown(row['summary'])\n",
    "                        if pd.notnull(row['skills']) and row['skills']:\n",
    "                            st.markdown(\"**Skills:**\")\n",
    "                            st.markdown(row['skills'])\n",
    "                        if pd.notnull(row['languages']) and row['languages']:\n",
    "                            st.markdown(\"**Languages:**\")\n",
    "                            st.markdown(row['languages'])\n",
    "                    \n",
    "                    if pd.notnull(row['experiences']) and row['experiences']:\n",
    "                        st.markdown(\"---\")\n",
    "                        st.markdown(\"### Experience Details\")\n",
    "                        experiences = row['experiences'].split('\\n')\n",
    "                        for exp in experiences:\n",
    "                            st.markdown(f\"- {exp}\")\n",
    "                    \n",
    "                    if pd.notnull(row['education']) and row['education']:\n",
    "                        st.markdown(\"---\")\n",
    "                        st.markdown(\"### Education Details\")\n",
    "                        educations = row['education'].split('\\n')\n",
    "                        for edu in educations:\n",
    "                            st.markdown(f\"- {edu}\")\n",
    "                    \n",
    "                    if pd.notnull(row['certifications']) and row['certifications']:\n",
    "                        st.markdown(\"---\")\n",
    "                        st.markdown(\"### Certifications\")\n",
    "                        certs = row['certifications'].split('\\n')\n",
    "                        for cert in certs:\n",
    "                            st.markdown(f\"- {cert}\")\n",
    "    with tab2:\n",
    "        st.header(\"Ranked Candidates\")\n",
    "        if st.session_state.ranked_results is not None and not st.session_state.ranked_results.empty:\n",
    "            export_columns = [\n",
    "                'full_name', 'country', 'country_full_name', 'province', 'city', \n",
    "                'personal_emails', 'personal_numbers', 'URL', 'gender', 'headline', \n",
    "                'summary', 'industry', 'experiences', 'education', 'skills', \n",
    "                'certifications', 'languages', 'similarity_score'\n",
    "            ]\n",
    "            export_df = st.session_state.ranked_results[\n",
    "                [col for col in export_columns if col in st.session_state.ranked_results.columns]\n",
    "            ].copy()\n",
    "            if 'similarity_score' in export_df.columns:\n",
    "                export_df['similarity_score'] = export_df['similarity_score'] * 100\n",
    "            excel_data = to_excel(export_df)\n",
    "            st.download_button(\n",
    "                label=\" Download Ranked Candidates (Excel)\",\n",
    "                data=excel_data,\n",
    "                file_name='ranked_candidates.xlsx',\n",
    "                mime='application/vnd.ms-excel',\n",
    "            )\n",
    "            \n",
    "            st.subheader(\"Match Results\")\n",
    "            top_candidates = st.session_state.ranked_results.head(10)\n",
    "            chart_data = pd.DataFrame({\n",
    "                'Candidate': top_candidates['full_name'],  # Changed from 'Name' to 'full_name'\n",
    "                'Match Percentage': top_candidates['similarity_score'] * 100\n",
    "            })\n",
    "            st.bar_chart(chart_data.set_index('Candidate'))\n",
    "            \n",
    "            for i, row in st.session_state.ranked_results.iterrows():\n",
    "                with st.expander(f\"{row['full_name']} - {row['headline']} (Match: {row['match_percentage']})\"):\n",
    "                    col1, col2 = st.columns([1, 2])\n",
    "                    with col1:\n",
    "                        st.markdown(f\"**Match Score:** {row['match_percentage']}\")\n",
    "                        st.markdown(f\"**Location:** {row['city']}\")\n",
    "                        st.markdown(f\"**Country:** {row['country']}\")\n",
    "                        st.markdown(f\"**Industry:** {row['industry']}\")\n",
    "                        st.markdown(f\"**Profile URL:** [Link]({row['URL']})\")\n",
    "                    with col2:\n",
    "                        if pd.notnull(row['summary']) and row['summary']:\n",
    "                            st.markdown(\"**Summary:**\")\n",
    "                            st.markdown(row['summary'])\n",
    "                        if pd.notnull(row['skills']) and row['skills']:\n",
    "                            st.markdown(\"**Skills:**\")\n",
    "                            st.markdown(row['skills'])\n",
    "                    if pd.notnull(row['experiences']) and row['experiences']:\n",
    "                        st.markdown(\"---\")\n",
    "                        st.markdown(\"### Experience Details\")\n",
    "                        experiences = row['experiences'].split('\\n')\n",
    "                        for exp in experiences:\n",
    "                            st.markdown(f\"- {exp}\")\n",
    "                    if pd.notnull(row['education']) and row['education']:\n",
    "                        st.markdown(\"---\")\n",
    "                        st.markdown(\"### Education Details\")\n",
    "                        educations = row['education'].split('\\n')\n",
    "                        for edu in educations:\n",
    "                            st.markdown(f\"- {edu}\")\n",
    "        else:\n",
    "            st.info(\"No ranked results available. Please search for candidates and rank them first.\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"\"\"\n",
    "    **How to use this application:**\n",
    "    1. Enter a job title and optionally other parameters in the search boxes (use OR for multiple terms)\n",
    "    2. Use slider to limit the number of results returned\n",
    "    3. Click \"Search Candidates\" to find matching profiles\n",
    "    4. Download the results as an Excel file\n",
    "    5. Optionally, enter a job description to match candidates against\n",
    "    6. Click \"Rank Candidates\" to sort candidates by relevance to the job description\n",
    "    7. View detailed rankings in the \"Ranked Results\" tab\n",
    "    8. Download the ranked candidates as an Excel file in the \"Ranked Results\" tab\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'public_identifier': 'hamza-joosub-a7b55a248', 'profile_pic_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/person/hamza-joosub-a7b55a248/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20250325%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20250325T173255Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=fc8127f330ed80461544c1c34b768f427aad2b65f791408651bfae47e5b9c6e7', 'background_cover_image_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/person/hamza-joosub-a7b55a248/cover?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20250325%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20250325T173255Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=9f60275062fc116901b49645d9da7e8bcfe2032db61418e5b72684fbf1e73321', 'first_name': 'Hamza', 'last_name': 'Joosub', 'full_name': 'Hamza Joosub', 'follower_count': 5, 'occupation': 'Statistical Analyst at Ayesha Suliman', 'headline': 'Student at University of Cape Town', 'summary': 'Highly analytical and detail-oriented data professional with a robust background in statistical analysis, data engineering, and financial modelling. Seeking to leverage expertise in Python and advanced data analytics to drive data-driven decision-making and research in the finance sector.', 'country': 'ZA', 'country_full_name': 'South Africa', 'city': 'Cape Town', 'state': 'Western Cape', 'experiences': [{'starts_at': {'day': 1, 'month': 12, 'year': 2024}, 'ends_at': {'day': 31, 'month': 1, 'year': 2025}, 'company': 'Ayesha Suliman', 'company_linkedin_profile_url': None, 'company_facebook_profile_url': None, 'title': 'Statistical Analyst', 'description': 'As a Freelance Data Analyst and Statistical Consultant, I assisted a PhD researcher in optometry with statistical analysis, data cleaning, and interpretation using SPSS and Python. My role involved:\\n\\nData Preparation & Cleaning  Processed raw research data, handled missing values, ensured data consistency, and standardized numerical variables for analysis.\\nExploratory Data Analysis (EDA)  Conducted descriptive statistics, visualized distributions, and identified patterns using SPSS and Python.\\nStatistical Analysis & Hypothesis Testing  Performed t-tests, ANOVA, chi-square tests, correlation analysis, and regression modeling to extract insights from research data.\\nSPSS & Python-Based Analytics  Leveraged SPSS for traditional statistical workflows while utilizing Python (pandas, scikit-learn, and matplotlib) for deeper exploratory analysis and efficiency in data processing.\\nReporting & Documentation  Generated structured reports with statistical findings, visualizations, and actionable insights, ensuring clarity for academic research.\\n\\nThis role strengthened my expertise in applied statistics, clinical data analysis, and research-driven insights, bridging the gap between statistical theory and practical implementation in healthcare research.\\n\" Muhammed demonstrated exceptional proficiency in statistical analysis, providing insightful and accurate assistance with complex data sets and advanced methodologies. \"', 'location': 'City of Johannesburg, Gauteng, South Africa', 'logo_url': None}, {'starts_at': {'day': 1, 'month': 4, 'year': 2024}, 'ends_at': {'day': 31, 'month': 5, 'year': 2024}, 'company': 'AHJ biokineticist', 'company_linkedin_profile_url': None, 'company_facebook_profile_url': None, 'title': 'Data Engineer', 'description': 'Data Extraction and Transformation: Engineered solutions to extract and preprocess EEG data from multiple patients, ensuring data integrity and usability for detailed neurological analysis.\\n\\nTechnical Implementation: Leveraged Python programming to efficiently handle and transform complex data sets, focusing on signal processing to compute alpha power values.\\n\\nResearch Enablement: Facilitated advanced neuroscience research by developing robust data pipelines that supported the extraction of meaningful insights from raw EEG recordings.\\n\\n\"Muhammed\\'s expertise in IT, particularly in coding with Python, played a pivotal role in the smooth execution of complex tasks.\"\\n\\n\"Muhammed is reliable, meets deadlines, and consistently delivers high-quality results. His problem-solving ability and logical approach to debugging and optimizing code not only saved time but also improved the efficiency of our data analysis.\"', 'location': 'City of Johannesburg, Gauteng, South Africa', 'logo_url': None}, {'starts_at': {'day': 1, 'month': 12, 'year': 2023}, 'ends_at': {'day': 31, 'month': 12, 'year': 2023}, 'company': 'Sentio Capital Management', 'company_linkedin_profile_url': 'https://www.linkedin.com/company/sentio-capital-management/', 'company_facebook_profile_url': None, 'title': 'Summer Intern', 'description': 'I had the opportunity to work with a talented team on refining financial models, including the Discounted Cash Flow (DCF) and DuPont models. My group and I contributed to optimising these models, ensuring they provided more accurate and actionable insights for investment analysis. This experience enhanced my understanding of financial metrics and allowed me to collaborate closely with professionals in asset management, further developing my skills in financial modeling and analysis.\"', 'location': 'City of Johannesburg, Gauteng, South Africa', 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/sentio-capital-management/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20250325%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20250325T173255Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=2de8dd7259b223fb8f9da80e2a1980ae504a7773cfd1647a1616c81a55c3f339'}], 'education': [{'starts_at': {'day': 1, 'month': 1, 'year': 2022}, 'ends_at': {'day': 31, 'month': 1, 'year': 2026}, 'field_of_study': 'Statistics and Data Science', 'degree_name': 'Bachelor of business science', 'school': 'University of Cape Town', 'school_linkedin_profile_url': 'https://www.linkedin.com/school/university-of-cape-town/', 'school_facebook_profile_url': None, 'description': None, 'logo_url': 'https://s3.us-west-000.backblazeb2.com/proxycurl/company/university-of-cape-town/profile?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=0004d7f56a0400b0000000001%2F20250325%2Fus-west-000%2Fs3%2Faws4_request&X-Amz-Date=20250325T173255Z&X-Amz-Expires=1800&X-Amz-SignedHeaders=host&X-Amz-Signature=64f2e9d69b8303144be766daf62d9ac34f23a0b64aa1f65507a8f7773a8f89cb', 'grade': None, 'activities_and_societies': None}], 'languages': [], 'languages_and_proficiencies': [], 'accomplishment_organisations': [], 'accomplishment_publications': [], 'accomplishment_honors_awards': [], 'accomplishment_patents': [], 'accomplishment_courses': [], 'accomplishment_projects': [{'starts_at': {'day': 1, 'month': 6, 'year': 2024}, 'ends_at': {'day': 30, 'month': 6, 'year': 2024}, 'title': \"Enhancing Traditional Financial Models with Machine Learning Techniques and LLM's for deeper Insights\", 'description': \"Financial Models(With LLM): Created traditional DCF & Multiples Valuation Model to determine the intrinsic and relative value of companies. The LLM(Llama 3: 7B) gives summaries of the model and critics the assumptions of the model allowing for refinement of the model.\\n\\nClustering Algorithm Enhancement: Integrated a clustering algorithm to refine Multiples Valuation Model, ensuring a precise selection of comparable companies.\\n\\nSector Analysis: Developed a sector screener that applies some traditional statistical techniques to find out better performing sectors according to certain metrics. The LLM(Llama 3: 7B) then summarises the data and provides insight as to the best sectors for investment depending on your risk profile\\n\\nAI-Driven Insights: Each Model uses an LLM (Llama 3: 7B) to provide a summary and deeper insights on the model's outputs and assumptions.\", 'url': None}], 'accomplishment_test_scores': [], 'volunteer_work': [], 'certifications': [{'starts_at': {'day': 1, 'month': 4, 'year': 2023}, 'ends_at': None, 'name': 'Machine Learning Specialization', 'license_number': '8PCH2B7UBFDC', 'display_source': 'coursera.org', 'authority': 'Coursera', 'url': 'https://www.coursera.org/account/accomplishments/specialization/certificate/8PCH2B7UBFDC'}], 'connections': 5, 'people_also_viewed': [{'link': 'https://www.linkedin.com/in/gavriel-burgin-2836b3257', 'name': 'Gavriel Burgin', 'summary': 'Master of Financial Engineering (AIFMRM) student | Actuarial Science BCom (Hons)', 'location': None}, {'link': 'https://www.linkedin.com/in/shannon-amoils-47b403319', 'name': 'Shannon Amoils', 'summary': 'Actuarial Science Student at University of Cape Town', 'location': None}, {'link': 'https://www.linkedin.com/in/daniel-stephens-34178111a', 'name': 'Daniel Stephens', 'summary': 'Derivatives Structuring & Trading FASSA', 'location': None}, {'link': 'https://www.linkedin.com/in/emma-hulton-191aa27', 'name': 'Emma Hulton', 'summary': 'Chief Operating Officer at Flagship Asset Management', 'location': None}, {'link': 'https://www.linkedin.com/in/david-white-74742a240', 'name': 'David White', 'summary': 'Derivatives Dealer and Quantitative Analyst', 'location': None}, {'link': 'https://www.linkedin.com/in/chiamaka-chijiokeagina-48b632320', 'name': 'Chiamaka ChijiokeAgina', 'summary': 'Bachelor of Business sciences, Analytics at University of Cape Town', 'location': None}, {'link': 'https://www.linkedin.com/in/gerbrandt-kruger', 'name': 'Gerbrandt Kruger', 'summary': 'Portfolio Manager | Fund Manager Research | Capital Markets', 'location': None}, {'link': 'https://www.linkedin.com/in/sfiso-sunduzwayo-130571266', 'name': 'Sfiso Sunduzwayo', 'summary': '', 'location': None}, {'link': 'https://www.linkedin.com/in/michael-bchle-232833323', 'name': 'Michael Bchle', 'summary': 'Derivatives Trader/Analyst at Constellation Capital', 'location': None}, {'link': 'https://www.linkedin.com/in/cameronfellner', 'name': 'Cameron Fellner', 'summary': 'MFE Student at AIFMRM | B.Eng. Computer Engineering', 'location': None}], 'recommendations': [], 'activities': [], 'similarly_named_profiles': [], 'articles': [], 'groups': [], 'skills': ['Statistics', 'Data Modeling', 'Scikit-Learn', 'Machine Learning', 'Plotly', 'Microsoft Excel', 'Financial Modeling', 'IBM SPSS', 'Data Analysis', 'R (Programming Language)', 'SQL', 'Java', 'Python (Programming Language)', 'Database Management System (DBMS)', 'Artificial Neural Networks', 'Recommender Systems', 'k-means clustering', 'Anomaly Detection', 'Reinforcement Learning', 'Decision Trees'], 'inferred_salary': None, 'gender': None, 'birth_date': None, 'industry': None, 'extra': None, 'interests': [], 'personal_emails': [], 'personal_numbers': []}\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Bearer QY5y_O7Axx4-fcXPEiTwmA' \n",
    "}\n",
    "\n",
    "api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "\n",
    "params = {\n",
    "    'linkedin_profile_url': 'https://www.linkedin.com/in/hamza-joosub-a7b55a248',\n",
    "    'personal_contact_number': 'include',\n",
    "    'personal_email': 'include',\n",
    "    'skills': 'include',\n",
    "    'use_cache': 'if-present',\n",
    "    'fallback_to_cache': 'on-error',\n",
    "}\n",
    "\n",
    "resp = requests.get(api_endpoint, headers=headers, params=params)\n",
    "resp.raise_for_status()  # Will raise an HTTPError for bad responses\n",
    "data = resp.json()\n",
    "print(data)  # Inspect the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 401, 'description': 'Invalid API key', 'name': 'Unauthorized'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "def search_employees_one_row_per_employee_dedup(\n",
    "    query,\n",
    "    country_filter=None,\n",
    "    location_filter=None,\n",
    "    university_filter=None,\n",
    "    province_filter=None,\n",
    "    skills_filter=None,\n",
    "    industry_filter=None,\n",
    "    company_filter=None,\n",
    "    languages_filter=None,\n",
    "    max_to_fetch=None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Calls the Proxycurl /search/person endpoint with the given parameters\n",
    "    and returns the first LinkedIn URL found in the response.\n",
    "    Raises ValueError if no results or no linkedin_profile_url found.\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer QY5y_O7Axx4-fcXPEiTwmA'}\n",
    "    search_endpoint = 'https://nubela.co/proxycurl/api/v2/search/person'\n",
    "    \n",
    "    search_params = {\n",
    "        'past_role_title': query,\n",
    "        'country': country_filter,\n",
    "        'city': location_filter,\n",
    "        'education_school_name': university_filter,\n",
    "        'region': province_filter,\n",
    "        'skills': skills_filter,\n",
    "        'industries': industry_filter,\n",
    "        'past_company_name': company_filter,\n",
    "        'languages': languages_filter,\n",
    "        'page_size': max_to_fetch,\n",
    "        'use_cache': 'if-present'\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(search_endpoint, headers=headers, params=search_params)\n",
    "    data = resp.json()\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "    if not results:\n",
    "        raise ValueError(\"No results returned from Search Person endpoint.\")\n",
    "    \n",
    "    linkedin_url = results[0].get(\"linkedin_profile_url\")\n",
    "    if not linkedin_url:\n",
    "        raise ValueError(\"No linkedin_profile_url found in the search result.\")\n",
    "    \n",
    "    return linkedin_url\n",
    "\n",
    "\n",
    "def enrich_profile(linkedin_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calls the Proxycurl /api/v2/linkedin endpoint to fetch detailed\n",
    "    LinkedIn profile data given a LinkedIn URL.\n",
    "    Returns the JSON (dictionary) with the user's profile data.\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer  QY5y_O7Axx4-fcXPEiTwmA'}\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "    \n",
    "    params = {\n",
    "        'linkedin_profile_url': linkedin_url,\n",
    "        'personal_contact_number': 'include',\n",
    "        'personal_email': 'include',\n",
    "        'skills': 'include',\n",
    "        'use_cache': 'if-present',\n",
    "        'fallback_to_cache': 'on-error',\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(api_endpoint, headers=headers, params=params)\n",
    "    data = resp.json()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def flatten_profile_data(profile_data: dict, linkedin_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given the enriched LinkedIn profile JSON and its LinkedIn URL,\n",
    "    return a single flat dictionary with pipe-delimited strings for\n",
    "    experiences, education, skills, certifications, etc.\n",
    "    Each subsequent line in experiences/education is prefixed with \"| \".\n",
    "    \"\"\"\n",
    "\n",
    "    def format_date(date_dict):\n",
    "        \"\"\"Return 'MonthName YYYY' or 'None' if missing/invalid.\"\"\"\n",
    "        if not date_dict or \"year\" not in date_dict or \"month\" not in date_dict:\n",
    "            return \"None\"\n",
    "        y = date_dict[\"year\"]\n",
    "        m = date_dict[\"month\"]\n",
    "        return f\"{calendar.month_name[m]} {y}\"\n",
    "\n",
    "    def compute_duration_months(start_dict, end_dict=None):\n",
    "        \"\"\"\n",
    "        Compute approximate duration in months between start_dict and end_dict.\n",
    "        If end_dict is None, use a 'today' date for demonstration (2025-03-25).\n",
    "        \"\"\"\n",
    "        if not start_dict or \"year\" not in start_dict or \"month\" not in start_dict:\n",
    "            return None\n",
    "        \n",
    "        start_year = start_dict[\"year\"]\n",
    "        start_month = start_dict[\"month\"]\n",
    "        \n",
    "        if end_dict and \"year\" in end_dict and \"month\" in end_dict:\n",
    "            end_year = end_dict[\"year\"]\n",
    "            end_month = end_dict[\"month\"]\n",
    "        else:\n",
    "            today = date(2025, 3, 25)\n",
    "            end_year = today.year\n",
    "            end_month = today.month\n",
    "        \n",
    "        start_total = start_year * 12 + (start_month - 1)\n",
    "        end_total   = end_year   * 12 + (end_month   - 1)\n",
    "        \n",
    "        return end_total - start_total\n",
    "\n",
    "    def multiline_pipe_format(lines):\n",
    "        \"\"\"\n",
    "        Given a list of strings (lines), return a single string\n",
    "        where each line after the first is prefixed by '| '.\n",
    "        \"\"\"\n",
    "        if not lines:\n",
    "            return \"\"\n",
    "        # First line as-is, subsequent lines prefixed with \"| \"\n",
    "        out = [lines[0]]\n",
    "        for line in lines[1:]:\n",
    "            out.append(f\"| {line}\")\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "    # Extract top-level fields\n",
    "    full_name         = profile_data.get(\"full_name\", \"\")\n",
    "    country           = profile_data.get(\"country\", \"\")\n",
    "    country_full_name = profile_data.get(\"country_full_name\", \"\")\n",
    "    # rename \"state\" to \"province\" in final data\n",
    "    province          = profile_data.get(\"state\", \"\")  \n",
    "    city              = profile_data.get(\"city\", \"\")\n",
    "    personal_emails   = profile_data.get(\"personal_emails\", [])\n",
    "    personal_numbers  = profile_data.get(\"personal_numbers\", [])\n",
    "    gender            = profile_data.get(\"gender\")\n",
    "    headline          = profile_data.get(\"headline\", \"\")\n",
    "    summary           = profile_data.get(\"summary\", \"\")\n",
    "    industry          = profile_data.get(\"industry\")\n",
    "    \n",
    "    # experiences, education, etc.\n",
    "    experiences = profile_data.get(\"experiences\", [])\n",
    "    education   = profile_data.get(\"education\", [])\n",
    "    skills_list = profile_data.get(\"skills\", [])\n",
    "    lang_list   = profile_data.get(\"languages\", [])\n",
    "    certs       = profile_data.get(\"certifications\", [])\n",
    "    \n",
    "    # Format experiences with trailing pipe\n",
    "    exp_lines = []\n",
    "    for exp in experiences:\n",
    "        title     = exp.get(\"title\", \"\")\n",
    "        company   = exp.get(\"company\", \"\")\n",
    "        start_d   = exp.get(\"starts_at\")\n",
    "        end_d     = exp.get(\"ends_at\")\n",
    "        \n",
    "        from_str  = format_date(start_d)\n",
    "        to_str    = format_date(end_d)\n",
    "        duration  = compute_duration_months(start_d, end_d)\n",
    "        duration_str = f\"{duration} months\" if duration is not None else \"Unknown\"\n",
    "        \n",
    "        # Note the pipe at the end\n",
    "        line = (\n",
    "            f\"Role: {title} | Company: {company} | From: {from_str} | \"\n",
    "            f\"To: {to_str} | Duration: {duration_str} |\"\n",
    "        )\n",
    "        exp_lines.append(line)\n",
    "    experiences_str = multiline_pipe_format(exp_lines)\n",
    "\n",
    "    # Format education with trailing pipe\n",
    "    edu_lines = []\n",
    "    for edu_item in education:\n",
    "        school  = edu_item.get(\"school\", \"\")\n",
    "        degree  = edu_item.get(\"degree_name\", \"\")\n",
    "        start_  = edu_item.get(\"starts_at\")\n",
    "        end_    = edu_item.get(\"ends_at\")\n",
    "        \n",
    "        from_edu = format_date(start_)\n",
    "        to_edu   = format_date(end_)\n",
    "        \n",
    "        line = (\n",
    "            f\"Institution: {school} | Degree: {degree} | \"\n",
    "            f\"From: {from_edu} | To: {to_edu} |\"\n",
    "        )\n",
    "        edu_lines.append(line)\n",
    "    education_str = multiline_pipe_format(edu_lines)\n",
    "\n",
    "    # Format skills, languages\n",
    "    skills_str    = \" | \".join(skills_list)\n",
    "    languages_str = \" | \".join(lang_list)\n",
    "\n",
    "    # Format certifications\n",
    "    cert_lines = []\n",
    "    for c in certs:\n",
    "        name      = c.get(\"name\", \"\")\n",
    "        authority = c.get(\"authority\", \"\")\n",
    "        start_c   = c.get(\"starts_at\")\n",
    "        end_c     = c.get(\"ends_at\")\n",
    "        from_c    = format_date(start_c)\n",
    "        to_c      = format_date(end_c)\n",
    "        \n",
    "        line = (\n",
    "            f\"Name: {name} | Authority: {authority} | \"\n",
    "            f\"From: {from_c} | To: {to_c} |\"\n",
    "        )\n",
    "        cert_lines.append(line)\n",
    "    certs_str = multiline_pipe_format(cert_lines)\n",
    "\n",
    "    # Build final record as a flat dict\n",
    "    record = {\n",
    "        \"full_name\":          full_name,\n",
    "        \"country\":            country,\n",
    "        \"country_full_name\":  country_full_name,\n",
    "        \"province\":           province,           \n",
    "        \"city\":               city,\n",
    "        \"personal_emails\":    \", \".join(personal_emails),\n",
    "        \"personal_numbers\":   \", \".join(personal_numbers),\n",
    "        \"URL\":                linkedin_url,\n",
    "        \"gender\":             gender,\n",
    "        \"headline\":           headline,\n",
    "        \"summary\":            summary,\n",
    "        \"industry\":           industry,\n",
    "        \"experiences\":        experiences_str,\n",
    "        \"education\":          education_str,\n",
    "        \"skills\":             skills_str,\n",
    "        \"certifications\":     certs_str,\n",
    "        \"languages\":          languages_str\n",
    "    }\n",
    "    return record\n",
    "\n",
    "\n",
    "def run_script(\n",
    "    query: str,\n",
    "    country_filter=None,\n",
    "    location_filter=None,\n",
    "    university_filter=None,\n",
    "    province_filter=None,\n",
    "    skills_filter=None,\n",
    "    industry_filter=None,\n",
    "    company_filter=None,\n",
    "    languages_filter=None,\n",
    "    max_to_fetch=None\n",
    ") -> pd.DataFrame:\n",
    "    linkedin_url = search_employees_one_row_per_employee_dedup(\n",
    "        query,\n",
    "        country_filter=country_filter,\n",
    "        location_filter=location_filter,\n",
    "        university_filter=university_filter,\n",
    "        province_filter=province_filter,\n",
    "        skills_filter=skills_filter,\n",
    "        industry_filter=industry_filter,\n",
    "        company_filter=company_filter,\n",
    "        languages_filter=languages_filter,\n",
    "        max_to_fetch=max_to_fetch\n",
    "    )\n",
    "\n",
    "    profile_json = enrich_profile(linkedin_url=linkedin_url)\n",
    "    record = flatten_profile_data(profile_data=profile_json, linkedin_url=linkedin_url)\n",
    "    \n",
    "    df_employees = pd.DataFrame([record])\n",
    "    return df_employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
